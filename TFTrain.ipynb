{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source directories\n",
    "s_dir = \"/home/vishnu/TFlow_ws/CameraProj/MLData/Smile\"\n",
    "o_dir = \"/home/vishnu/TFlow_ws/CameraProj/MLData/Other\"\n",
    "\n",
    "#Log dir\n",
    "LOGDIR = \"/home/vishnu/TFlow_ws/CameraProj/src/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image Preprocessing functions\n",
    "def get_files(Sdir,Odir):\n",
    "    filelist = [Sdir+\"/\"+x for x in os.listdir(Sdir)]+[Odir+\"/\"+y for y in os.listdir(Odir)]\n",
    "    shuffle(filelist)\n",
    "    label = [1 if x.find('Smile') != -1 else 0 for x in filelist]\n",
    "    return filelist, label\n",
    "\n",
    "#Parses function for data_array iterator\n",
    "def _parse_fun(filename,labels):\n",
    "    image_string = tf.read_file(filename,name=\"read_files\")\n",
    "    image_decode = tf.image.decode_jpeg(image_string,channels=1)\n",
    "    image = tf.cast(image_decode,tf.float32,name=\"cast_f32\")\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    one_hot = tf.one_hot(labels,2)\n",
    "    \n",
    "    #return dataset,labels\n",
    "    return image,one_hot\n",
    "\n",
    "#Parse function for data augment iterator\n",
    "def data_augment(filename,labels):\n",
    "    \n",
    "    image_string = tf.read_file(filename,name=\"read_files_data_aug\")\n",
    "    image_decode = tf.image.decode_jpeg(image_string,channels=1)\n",
    "    image = tf.cast(image_decode,tf.float32,name=\"cast_image_f32\")\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    \n",
    "    #Creating image mirror\n",
    "    mirror_img = tf.image.flip_left_right(image)\n",
    "    one_hot = tf.one_hot(labels,2)\n",
    "    \n",
    "    #Fine angle rotations\n",
    "    \n",
    "    \n",
    "    #Scaling\n",
    "    \n",
    "    #Noise\n",
    "    return [image,mirror_img], one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining some functions for developing network model\n",
    "\n",
    "#Weight_variables\n",
    "def weight_variable(shape,m_name):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1)#.1\n",
    "    return tf.Variable(initial,name=m_name)\n",
    "\n",
    "#Bias Variable\n",
    "def bias_variable(shape,m_name):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(initial,name=m_name)\n",
    "\n",
    "#Conv\n",
    "def conv2d(x,W,m_name):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME',name=m_name)\n",
    "\n",
    "#max_pool_5x5\n",
    "def max_pool(x,m_ksize,m_kstrides,m_name):\n",
    "    return tf.nn.max_pool(x,ksize=[1,m_ksize,m_ksize,1],strides=[1,m_kstrides,m_kstrides,1],padding='SAME',name=m_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Augmentation section\n",
    "\n",
    "#Reading file names\n",
    "filename,labels = get_files(s_dir,o_dir)\n",
    "\n",
    "#Creating Dataset\n",
    "dataset_aug = tf.data.Dataset.from_tensor_slices((filename,labels))\n",
    "#Processing data with the data_augment function\n",
    "dataset_aug = dataset_aug.map(data_augment)\n",
    "\n",
    "#Declaring iterator\n",
    "it = tf.data.Iterator.from_structure(dataset_aug.output_types,dataset_aug.output_shapes)\n",
    "it_op = it.make_initializer(dataset_aug)\n",
    "\n",
    "x,y_ = it.get_next()\n",
    "#Variable initialize operation\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#Data Array contains the augmented data.\n",
    "data_array = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    sess.run(it_op)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            rtn,yy = sess.run([x,y_])\n",
    "            data_array.append([rtn[0],yy])\n",
    "            data_array.append([rtn[1],yy])\n",
    "            \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Finished\")\n",
    "            break\n",
    "shuffle(data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2660, 1326)\n"
     ]
    }
   ],
   "source": [
    "length = len(data_array)\n",
    "third = 2*int(length/3)\n",
    "\n",
    "data_array = np.array(data_array)\n",
    "\n",
    "Train_data = data_array[:third][:,0]\n",
    "Train_data_labels = data_array[:third][:,1]\n",
    "Test_data = data_array[third:][:,0]\n",
    "Test_data_labels = data_array[third:][:,1]\n",
    "\n",
    "x = tf.placeholder(tf.float32,shape=[50,50,1],name=\"x\")\n",
    "y_ = tf.placeholder(tf.float32,shape=[1,2],name=\"y_\")\n",
    "\n",
    "count = 0;\n",
    "for i in range(len(Train_data_labels)):\n",
    "    if (Train_data_labels[i][0] == 1):\n",
    "        count = count+1\n",
    "\n",
    "print(len(Train_data_labels),count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 735]\n"
     ]
    }
   ],
   "source": [
    "k_size1 = 5\n",
    "k_size2 = 2\n",
    "k_size3 = 2\n",
    "conv1_feat = 5 #32\n",
    "conv2_feat = 10\n",
    "conv3_feat = 15\n",
    "pool1 = 2\n",
    "pool2 = 2\n",
    "pool3 = 2\n",
    "\n",
    "#Convolution layers\n",
    "#Layer 1 variables\n",
    "w_conv1 = weight_variable([k_size1,k_size1,1,conv1_feat],\"w_conv1\")\n",
    "b_conv1 = bias_variable([conv1_feat],\"b_conv1\")\n",
    "\n",
    "#Layer 2 variables\n",
    "w_conv2 = weight_variable([k_size2,k_size2,conv1_feat,conv2_feat],\"w_conv2\")\n",
    "b_conv2 = bias_variable([conv2_feat],\"b_conv2\")\n",
    "\n",
    "#Layer 3 Variables\n",
    "w_conv3 = weight_variable([k_size3,k_size3,conv2_feat,conv3_feat],\"w_conv3\")\n",
    "b_conv3 = bias_variable([conv3_feat],\"b_conv3\")\n",
    "\n",
    "x_img = tf.reshape(x,[1,50,50,1],name=\"Reshaping_x\")\n",
    "with tf.name_scope(\"Input_image\"):\n",
    "    tf.summary.image(\"input_image\",x_img,1)\n",
    "\n",
    "with tf.name_scope(\"Conv1/Pool1\"):\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_img,w_conv1,\"relu_hconv1\")+b_conv1,name=\"h_conv1\")\n",
    "    h_pool1 = max_pool(h_conv1,pool1,pool1,\"h_pool1\")\n",
    "\n",
    "with tf.name_scope(\"Conv2/Pool2\"):\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1,w_conv2,\"relu_h_conv2\")+b_conv2)\n",
    "    h_pool2 = max_pool(h_conv2,pool2,pool2,\"h_pool2\")\n",
    "\n",
    "with tf.name_scope(\"Conv3/Pool3\"):\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2,w_conv3,\"relu_h_conv3\")+b_conv3)\n",
    "    h_pool3 = max_pool(h_conv3,pool3,pool3,\"h_pool3\")\n",
    "    h_pool3_shape = h_pool3.get_shape().as_list()\n",
    "    h_pool3_flat = tf.reshape(h_pool3,[1,h_pool3_shape[1]*h_pool3_shape[2]*conv3_feat])\n",
    "    \n",
    "print(h_pool3_flat.get_shape().as_list())\n",
    "input_nodes = h_pool3_flat.get_shape().as_list()\n",
    "\n",
    "fc2_i_nodes = 735\n",
    "fc2_o_nodes = 367\n",
    "fc3_i_nodes = 183\n",
    "fc3_o_nodes = 91\n",
    "fc4_i_nodes = 45\n",
    "\n",
    "#fc2_i_nodes = 3000\n",
    "#fc2_o_nodes = 1500\n",
    "#fc3_i_nodes = 750\n",
    "#fc3_o_nodes = 375\n",
    "#fc4_i_nodes = 187\n",
    "\n",
    "#Fully connected Layers\n",
    "#FC 1\n",
    "w_fc1 = weight_variable([input_nodes[1],fc2_i_nodes],\"w_fc1\")\n",
    "b_fc1 = bias_variable([fc2_i_nodes],\"b_fc1\")\n",
    "\n",
    "#FC 2\n",
    "w_fc2 = weight_variable([fc2_i_nodes,fc2_o_nodes],\"w_fc2\")\n",
    "b_fc2 = bias_variable([fc2_o_nodes],\"b_fc2\")\n",
    "\n",
    "#FC 3\n",
    "w_fc3 = weight_variable([fc2_o_nodes,fc3_i_nodes],\"w_fc3\")\n",
    "b_fc3 = bias_variable([fc3_i_nodes],\"b_fc3\")\n",
    "\n",
    "#FC 4\n",
    "w_fc4 = weight_variable([fc3_i_nodes,fc3_o_nodes],\"w_fc4\")\n",
    "b_fc4 = bias_variable([fc3_o_nodes],\"b_fc3\")\n",
    "\n",
    "#FC 5\n",
    "w_fc5 = weight_variable([fc3_o_nodes,2],\"w_fc4\")\n",
    "b_fc5 = bias_variable([2],\"b_fc3\")\n",
    "\n",
    "#Keep probability\n",
    "keep_prob = tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Full_Con1\"):\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat,w_fc1)+b_fc1)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob,name=\"h_fc1_drop\")\n",
    "\n",
    "with tf.name_scope(\"Full_Con2\"):\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop,w_fc2)+b_fc2)\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc2,keep_prob,name=\"h_fc2_drop\")\n",
    "\n",
    "with tf.name_scope(\"Full_Con3\"):\n",
    "    h_fc3 = tf.nn.relu(tf.matmul(h_fc2_drop,w_fc3)+b_fc3)\n",
    "    h_fc3_drop = tf.nn.dropout(h_fc3,keep_prob,name=\"h_fc3_drop\")\n",
    "    \n",
    "with tf.name_scope(\"Full_Con4\"):\n",
    "    h_fc4 = tf.nn.relu(tf.matmul(h_fc3_drop,w_fc4)+b_fc4)\n",
    "    h_fc4_drop = tf.nn.dropout(h_fc4,keep_prob,name=\"h_fc4_drop\")\n",
    "\n",
    "with tf.name_scope(\"Model_Output\"):\n",
    "    y_conv =  tf.matmul(h_fc4_drop,w_fc5,name=\"y_conv\")+b_fc5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Training_Unit\"):\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_,logits=y_conv,name=\"loss_fun\")\n",
    "    cross_entropy = tf.reduce_mean(loss)\n",
    "    tf.summary.scalar(\"loss\",cross_entropy)\n",
    "    train_step = tf.train.AdamOptimizer(.1).minimize(cross_entropy)\n",
    "    \n",
    "with tf.name_scope(\"correct_predict\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1),tf.argmax(y_,1),name=\"prediction\")\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32),name=\"accuracy\")\n",
    "    tf.summary.scalar(\"accuracy\",accuracy)\n",
    "    \n",
    "init_op = tf.global_variables_initializer()\n",
    "summ = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(LOGDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.490990990991\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    cor = 0\n",
    "    for i in range(len(Train_data)):\n",
    "        l = Train_data_labels[i].reshape(1,2)\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            [train_accuracy,s] = sess.run([accuracy,summ],feed_dict={x: Train_data[i],y_: l,keep_prob:1})\n",
    "            writer.add_summary(s,i)\n",
    "        \n",
    "        a = sess.run([train_step],feed_dict={x: Train_data[i],y_: l,keep_prob: 1})\n",
    "    \n",
    "    for i in range(len(Test_data)):\n",
    "        l = Test_data_labels[i].reshape(1,2)\n",
    "        cor_pred = sess.run(correct_prediction,feed_dict={x: Test_data[i],y_: l,keep_prob: 1})\n",
    "        if (cor_pred):\n",
    "            cor = cor+1\n",
    "        \n",
    "    writer.add_graph(sess.graph)   \n",
    "writer.close()\n",
    "\n",
    "print(float(cor)/len(Test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfilenames,labels = get_files(s_dir,o_dir)\\n\\nlength = len(filenames)\\nthird = int(length/3.0)\\n\\nTrain_names = filenames[:third]\\nTrain_labels = labels[:third]\\nTest_names = filenames[third:]\\nTest_labels = labels[third:]\\n\\ndataset_train = tf.data.Dataset.from_tensor_slices((Train_names,Train_labels))\\ndataset_train = dataset_train.map(_parse_fun)\\n#dataset_train = dataset_train.batch(10)\\n\\n\\ndataset_test = tf.data.Dataset.from_tensor_slices((Test_names,Test_labels))\\ndataset_test = dataset_test.map(_parse_fun)\\n#dataset_test = dataset_test.batch(10)\\n\\n\\niterator = tf.data.Iterator.from_structure(dataset_train.output_types,dataset_train.output_shapes)\\n\\ntraining_init_op = iterator.make_initializer(dataset_train)\\ntesting_init_op = iterator.make_initializer(dataset_test)\\n\\nx, y_ = iterator.get_next()'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "filenames,labels = get_files(s_dir,o_dir)\n",
    "\n",
    "length = len(filenames)\n",
    "third = int(length/3.0)\n",
    "\n",
    "Train_names = filenames[:third]\n",
    "Train_labels = labels[:third]\n",
    "Test_names = filenames[third:]\n",
    "Test_labels = labels[third:]\n",
    "\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((Train_names,Train_labels))\n",
    "dataset_train = dataset_train.map(_parse_fun)\n",
    "#dataset_train = dataset_train.batch(10)\n",
    "\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((Test_names,Test_labels))\n",
    "dataset_test = dataset_test.map(_parse_fun)\n",
    "#dataset_test = dataset_test.batch(10)\n",
    "\n",
    "\n",
    "iterator = tf.data.Iterator.from_structure(dataset_train.output_types,dataset_train.output_shapes)\n",
    "\n",
    "training_init_op = iterator.make_initializer(dataset_train)\n",
    "testing_init_op = iterator.make_initializer(dataset_test)\n",
    "\n",
    "x, y_ = iterator.get_next()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncal = [[1 ,1],[1 ,1]]\\ncal[0][0] = 0\\ncal[0][1] = 0\\ncal[1][0] = 0\\ncal[1][1] = 0\\nwith tf.Session() as sess:\\n    sess.run(init_op)\\n    sess.run(training_init_op)\\n    #tf.summary.FileWriter(\"logs\",sess.graph).close()\\n    while True:\\n        try:\\n            #img = sess.run(Train_image)\\n            sess.run([x,y_])\\n            sess.run(train_step,feed_dict={keep_prob: 0.5})\\n        except tf.errors.OutOfRangeError:\\n            print(\"Finished Training\")\\n            break\\n    \\n    sess.run(testing_init_op)\\n    \\n    while True:\\n        try:\\n            t_img,t_label = sess.run([x,y_])\\n            l = t_label\\n            val = sess.run(accuracy, feed_dict={keep_prob: 1.0})\\n            if (l.argmax() == val):\\n                cal[0][0] = cal[0][0]+1\\n            elif l.argmax() == 1 and val == 0:\\n                cal[1][0] = cal[1][0]+1\\n            elif l.argmax() == 0 and val == 1:\\n                cal[0][1] = cal[0][1]+1\\n            else:\\n                cal[1][1] = cal[1][1]+1\\n        except tf.errors.OutOfRangeError:\\n            print(\"Finished Testing\")\\n            break\\n                \\nprint(cal)\\nN = len(Test_labels)\\nprint(N)\\ntotal_correct = cal[0][0]+cal[1][1]\\nacc = (float(total_correct)/N)\\nprint(acc)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "cal = [[1 ,1],[1 ,1]]\n",
    "cal[0][0] = 0\n",
    "cal[0][1] = 0\n",
    "cal[1][0] = 0\n",
    "cal[1][1] = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    sess.run(training_init_op)\n",
    "    #tf.summary.FileWriter(\"logs\",sess.graph).close()\n",
    "    while True:\n",
    "        try:\n",
    "            #img = sess.run(Train_image)\n",
    "            sess.run([x,y_])\n",
    "            sess.run(train_step,feed_dict={keep_prob: 0.5})\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Finished Training\")\n",
    "            break\n",
    "    \n",
    "    sess.run(testing_init_op)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            t_img,t_label = sess.run([x,y_])\n",
    "            l = t_label\n",
    "            val = sess.run(accuracy, feed_dict={keep_prob: 1.0})\n",
    "            if (l.argmax() == val):\n",
    "                cal[0][0] = cal[0][0]+1\n",
    "            elif l.argmax() == 1 and val == 0:\n",
    "                cal[1][0] = cal[1][0]+1\n",
    "            elif l.argmax() == 0 and val == 1:\n",
    "                cal[0][1] = cal[0][1]+1\n",
    "            else:\n",
    "                cal[1][1] = cal[1][1]+1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Finished Testing\")\n",
    "            break\n",
    "                \n",
    "print(cal)\n",
    "N = len(Test_labels)\n",
    "print(N)\n",
    "total_correct = cal[0][0]+cal[1][1]\n",
    "acc = (float(total_correct)/N)\n",
    "print(acc)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
