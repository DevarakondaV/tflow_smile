{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from random import shuffle\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directories\n",
    "s_dir = \"/home/vishnu/TFlow_ws/CameraProj/MLData/Smile\"\n",
    "o_dir = \"/home/vishnu/TFlow_ws/CameraProj/MLData/Other\"\n",
    "#s_dir = r\"C:\\Users\\Vishnu\\Desktop\\MLData\\Smile\"\n",
    "#o_dir = r\"C:\\Users\\Vishnu\\Desktop\\MLData\\Other\"\n",
    "\n",
    "#Log dir\n",
    "LOGDIR = \"/home/vishnu/TFlow_ws/CameraProj/src/logs\"\n",
    "#LOGDIR = r\"C:\\Users\\Vishnu\\Desktop\\MLData\\logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(Sdir,Odir):\n",
    "    filelist = [Sdir+\"/\"+x for x in os.listdir(Sdir)]+[Odir+\"/\"+y for y in os.listdir(Odir)]\n",
    "    shuffle(filelist)\n",
    "    label = [1 if x.find('Smile') != -1 else 0 for x in filelist]\n",
    "    return filelist, label\n",
    "    \n",
    "def data_aug(filenames,labels):\n",
    "    img_string = tf.read_file(filenames,name=\"data_aug_read_files\")\n",
    "    img_decode = tf.image.decode_jpeg(img_string,channels=1)\n",
    "    img = tf.cast(img_decode,tf.float32,name=\"cast_float_32\")\n",
    "    img = tf.image.per_image_standardization(img)\n",
    "    \n",
    "    mirror_img = tf.image.flip_left_right(img)\n",
    "    one_hot = tf.one_hot(labels,2)\n",
    "    \n",
    "    \n",
    "    return [img,mirror_img],one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up data for model\n",
    "filenames,labels = get_files(s_dir,o_dir)\n",
    "dataset_aug = tf.data.Dataset.from_tensor_slices((filenames,labels))\n",
    "dataset_aug = dataset_aug.map(data_aug)\n",
    "\n",
    "it = tf.data.Iterator.from_structure(dataset_aug.output_types,dataset_aug.output_shapes)\n",
    "it_op = it.make_initializer(dataset_aug)\n",
    "\n",
    "x,y_ = it.get_next()\n",
    "#Variable initialize operation\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n",
      "(2660, 1325)\n"
     ]
    }
   ],
   "source": [
    "#Parsing Data\n",
    "data_array = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    sess.run(it_op)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            rtn,yy = sess.run([x,y_])\n",
    "            data_array.append([rtn[0],yy])\n",
    "            data_array.append([rtn[1],yy])\n",
    "            \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Finished\")\n",
    "            break\n",
    "shuffle(data_array)\n",
    "\n",
    "#Splitting data into train and other\n",
    "length = len(data_array)\n",
    "third = 2*int(length/3.0)\n",
    "\n",
    "data_array = np.array(data_array)\n",
    "Train_data = data_array[:third][:,0]\n",
    "Train_data_labels = data_array[:third][:,1]\n",
    "Test_data = data_array[third:][:,0]\n",
    "Test_data_labels = data_array[third:][:,1]\n",
    "\n",
    "count = 0;\n",
    "for i in range(len(Train_data_labels)):\n",
    "    if (Train_data_labels[i][0] == 1):\n",
    "        count = count+1\n",
    "\n",
    "print(len(Train_data_labels),count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tflow functions\n",
    "\n",
    "def conv_layer(m_input,size_in,size_out,k_size_w,k_size_h,pool_k_size,pool_stride_size,name,num):\n",
    "    with tf.name_scope(name+num):\n",
    "        w = tf.Variable(tf.truncated_normal([k_size_w,k_size_h,size_in,size_out],stddev = 0.1),name=(\"w\"+num))\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[size_out]),name=(\"b\"+num))\n",
    "        \n",
    "        conv = tf.nn.conv2d(m_input,w,strides=[1,1,1,1],padding=\"SAME\")\n",
    "        act = tf.nn.leaky_relu((conv+b),alpha=0.3)\n",
    "        #.3\n",
    "        \n",
    "        tf.summary.histogram(\"weights\",w)\n",
    "        tf.summary.histogram(\"biases\",b)\n",
    "        tf.summary.histogram(\"activations\",act)\n",
    "        return tf.nn.max_pool(act,ksize=[1,pool_k_size,pool_k_size,1],strides=[1,pool_stride_size,pool_stride_size,1],padding=\"SAME\")\n",
    "    \n",
    "def fc_layer(m_input,size_in,size_out,name,num):\n",
    "    with tf.name_scope(name+num):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in,size_out],stddev=0.1),name=(\"w\"+num))\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[size_out]),name=(\"b\"+num))\n",
    "        act = tf.nn.leaky_relu(tf.matmul(m_input,w)+b,alpha=0.3)\n",
    "\n",
    "        tf.summary.histogram(\"weights\",w)\n",
    "        tf.summary.histogram(\"biases\",b)\n",
    "        tf.summary.histogram(\"activations\",act)\n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model function\n",
    "\n",
    "def model(start_learning_rate,lr_decay,conv_count,fc_count,conv_feats,fc_feats,hparam):\n",
    "    global LOGDIR\n",
    "    global Train_data\n",
    "    if (len(conv_feats) != conv_count):\n",
    "        return\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    with tf.name_scope(\"place_holders\"):\n",
    "        x = tf.placeholder(tf.float32,shape=[50,50,1],name=\"x\")\n",
    "        x_image = tf.reshape(x,[-1,50,50,1])\n",
    "        y = tf.placeholder(tf.float32,shape=[None,2],name=\"y\")\n",
    "    \n",
    "    with tf.name_scope(\"variable\"):\n",
    "        learning_rt = tf.Variable(start_learning_rate,name=\"learning_rt\")\n",
    "        global_step = tf.Variable(0,trainable=False)\n",
    "        tf.summary.scalar(\"Learning_rate\",learning_rt)\n",
    "        \n",
    "    with tf.name_scope(\"lr_decay\"):\n",
    "        dec_learning_rate = tf.train.exponential_decay(learning_rt,global_step,len(Train_data),lr_decay,staircase=True)\n",
    "        \n",
    "        \n",
    "    \n",
    "    tf.summary.image(\"image_input\",x_image,1)\n",
    "    \n",
    "    convs = []\n",
    "    convs.append(x_image)\n",
    "    conv_name = \"conv\"\n",
    "    for i in range(0,conv_count-1):\n",
    "        convs.append(conv_layer(convs[i],conv_feats[i],conv_feats[i+1],5,5,2,2,\"conv\",str(i+1)))\n",
    "    #print((conv_layer(convs[0],1,conv_feats[0],5,5,2,2,\"conv\",str(0+1)).shape))\n",
    "        \n",
    "    \n",
    "    shape = (convs[conv_count-1]).get_shape().as_list()\n",
    "    fc_feats[0] = shape[1]*shape[2]*conv_feats[conv_count-1]\n",
    "    flatten = tf.reshape(convs[conv_count-1],[1,fc_feats[0]])\n",
    "    \n",
    "    fcs = []\n",
    "    fcs.append(flatten)\n",
    "    fcs_name = \"FC\"\n",
    "    for i in range(0,fc_count-1):\n",
    "        fcs.append((fc_layer(fcs[i],fc_feats[i],fc_feats[i+1],\"FC\",str(i+1))))\n",
    "    \n",
    "    \n",
    "    logts = fcs[len(fcs)-1]\n",
    "\n",
    "    with tf.name_scope(\"cross_entropy\"):\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logts,labels=y),name=\"cross_entropy\")\n",
    "        tf.summary.scalar(\"cross_entropy\",cross_entropy)\n",
    "\n",
    "    with tf.name_scope(\"Train\"):\n",
    "        train_step = tf.train.AdamOptimizer(dec_learning_rate).minimize(cross_entropy)\n",
    "    \n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        pred = tf.equal(tf.argmax(logts,1),tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(pred,tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\",accuracy)\n",
    "    \n",
    "    summ = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(LOGDIR+hparam)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        cor = 0\n",
    "        for i in range(len(Train_data)):\n",
    "            l = Train_data_labels[i].reshape(1,2)\n",
    "        \n",
    "            if i % 5 == 0:\n",
    "                [train_accuracy,s] = sess.run([accuracy,summ],feed_dict={x: Train_data[i],y: l})\n",
    "                writer.add_summary(s,i)\n",
    "        \n",
    "            a = sess.run([train_step],feed_dict={x: Train_data[i],y: l})\n",
    "    \n",
    "        for i in range(len(Test_data)):\n",
    "            l = Test_data_labels[i].reshape(1,2)\n",
    "            cor_pred = sess.run(pred,feed_dict={x: Test_data[i],y: l})\n",
    "            if (cor_pred):\n",
    "                cor = cor+1\n",
    "        \n",
    "        writer.add_graph(sess.graph)   \n",
    "        writer.close()\n",
    "\n",
    "        print(float(cor)/len(Test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.795795795796\n"
     ]
    }
   ],
   "source": [
    "#Feats that worked\n",
    "#75\n",
    "#rt = 1e-4\n",
    "#conv_feats=[1,16,32,64]->between 8-16\n",
    "#fc_feats=[0,1500,500,50,2] -> Too many neurons causing model not to train at all\n",
    "#77%\n",
    "#rt = 1e-4\n",
    "#conv_feats=[1,8,16,32]\n",
    "#fc_feats=[0,1000,500,50,2]\n",
    "#77%\n",
    "#rt = 1e-4\n",
    "#conv_feats=[1,8,12,16]\n",
    "#fc_feats=[0,700,300,100,2]\n",
    "#77%\n",
    "#rt = 1e-4\n",
    "#conv_feats=[1,8,10,12]\n",
    "#fc_feats=[0,700,300,150,2]\n",
    "#79%\n",
    "#rt = 1e-4\n",
    "#conv_feats=[1,8,10,12]\n",
    "#fc_feats=[0,300,150,50,2]\n",
    "#78%\n",
    "#conv_feats=[1,3,8,12]\n",
    "#fc_feats=[0,100,100,100,2]\n",
    "#82% lnrt=1e-3 dec = .6\n",
    "#conv_feats=[1,8,10,12]\n",
    "#fc_feats=[0,150,50,2]\n",
    "len_rt= 1e-3\n",
    "conv_count = 4\n",
    "fc_count = 4\n",
    "conv_feats=[1,8,10,12]\n",
    "#fc_feats=[0,300,150,50,2]\n",
    "fc_feats=[0,150,50,2]\n",
    "model(len_rt,.6,conv_count,fc_count,conv_feats,fc_feats,\"test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
