{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directories\n",
    "s_dir = \"/home/vishnu/TFlow_ws/CameraProj/MLData/Smile\"\n",
    "o_dir = \"/home/vishnu/TFlow_ws/CameraProj/MLData/Other\"\n",
    "#s_dir = r\"C:\\Users\\Vishnu\\Desktop\\MLData\\Smile\"\n",
    "#o_dir = r\"C:\\Users\\Vishnu\\Desktop\\MLData\\Other\"\n",
    "\n",
    "#Log dir\n",
    "LOGDIR = \"/home/vishnu/TFlow_ws/CameraProj/src/logs/\"\n",
    "#LOGDIR = r\"C:\\Users\\Vishnu\\Desktop\\MLData\\logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(Sdir,Odir):\n",
    "    filelist = [Sdir+\"/\"+x for x in os.listdir(Sdir)]+[Odir+\"/\"+y for y in os.listdir(Odir)]\n",
    "    shuffle(filelist)\n",
    "    label = [1 if x.find('Smile') != -1 else 0 for x in filelist]\n",
    "    return filelist, label\n",
    "    \n",
    "def data_aug(filenames,labels):\n",
    "    img_string = tf.read_file(filenames,name=\"data_aug_read_files\")\n",
    "    img_decode = tf.image.decode_jpeg(img_string,channels=1)\n",
    "    img = tf.cast(img_decode,tf.float32,name=\"cast_float_32\")\n",
    "    img = tf.image.per_image_standardization(img)\n",
    "    \n",
    "    mirror_img = tf.image.flip_left_right(img)\n",
    "    one_hot = tf.one_hot(labels,2)\n",
    "    \n",
    "    \n",
    "    return img,mirror_img,one_hot\n",
    "\n",
    "def train_data_map(x,y):\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up data for model\n",
    "filenames,labels = get_files(s_dir,o_dir)\n",
    "dataset_aug = tf.data.Dataset.from_tensor_slices((filenames,labels))\n",
    "dataset_aug = dataset_aug.map(data_aug)\n",
    "#dataset_aug.shuffle()\n",
    "it = tf.data.Iterator.from_structure(dataset_aug.output_types,dataset_aug.output_shapes)\n",
    "it_op = it.make_initializer(dataset_aug)\n",
    "\n",
    "\n",
    "x1,x2,yy = it.get_next()\n",
    "#ims = tf.stack([x1,x2],0)\n",
    "#labs = tf.stack([y_,y_],0)\n",
    "#btc = tf.concat([x1,y_],0)\n",
    "#Variable initialize operation\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n",
      "(6400, 50, 50, 1)\n",
      "(6400, 2)\n"
     ]
    }
   ],
   "source": [
    "img_stk = []\n",
    "label_stk = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    sess.run(it_op)\n",
    "    \n",
    "    while(True):\n",
    "        try:\n",
    "            r1,r2,r3 = sess.run([x1,x2,yy])\n",
    "            img_stk.append(r1)\n",
    "            img_stk.append(r2)\n",
    "            label_stk.append(r3)\n",
    "            label_stk.append(r3)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Finished\")\n",
    "            break\n",
    "\n",
    "print(np.asarray(img_stk).shape)\n",
    "print(np.asarray(label_stk).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800\n",
      "4800\n",
      "1600\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "#split arrayinto training and other\n",
    "length = len(img_stk)\n",
    "t_f = 3*int(length/4.0)\n",
    "\n",
    "img_stk = np.asarray(img_stk)\n",
    "label_stk=  np.asarray(label_stk)\n",
    "Tr_data = img_stk[:t_f]\n",
    "Tr_data_lab = label_stk[:t_f]\n",
    "Te_data = img_stk[t_f:]\n",
    "Te_data_lab = label_stk[t_f:]\n",
    "print(len(Tr_data))\n",
    "print(len(Tr_data_lab))\n",
    "print(len(Te_data))\n",
    "print(len(Te_data_lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tflow functions\n",
    "\n",
    "def conv_layer(m_input,size_in,size_out,k_size_w,k_size_h,pool_k_size,pool_stride_size,name,num):\n",
    "    with tf.name_scope(name+num):\n",
    "        w = tf.Variable(tf.truncated_normal([k_size_w,k_size_h,size_in,size_out],stddev = 0.1),name=(\"w\"+num))\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[size_out]),name=(\"b\"+num))\n",
    "        \n",
    "        conv = tf.nn.conv2d(m_input,w,strides=[1,1,1,1],padding=\"SAME\")\n",
    "        act = tf.nn.leaky_relu((conv+b),alpha=0.3)\n",
    "        #.3\n",
    "        \n",
    "        tf.summary.histogram(\"weights\",w)\n",
    "        tf.summary.histogram(\"biases\",b)\n",
    "        tf.summary.histogram(\"activations\",act)\n",
    "        return tf.nn.max_pool(act,ksize=[1,pool_k_size,pool_k_size,1],strides=[1,pool_stride_size,pool_stride_size,1],padding=\"SAME\")\n",
    "    \n",
    "def fc_layer(m_input,size_in,size_out,name,num):\n",
    "    with tf.name_scope(name+num):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in,size_out],stddev=0.1),name=(\"w\"+num))\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[size_out]),name=(\"b\"+num))\n",
    "        z = tf.matmul(m_input,w)+b\n",
    "        #bt_norm = batch_norm_fc(z,num)\n",
    "        #act = tf.nn.leaky_relu(bt_norm,alpha=0.3)\n",
    "        act = tf.nn.leaky_relu(z,alpha=0.3)\n",
    "        \n",
    "        tf.summary.histogram(\"weights\",w)\n",
    "        tf.summary.histogram(\"biases\",b)\n",
    "        tf.summary.histogram(\"activations\",act)\n",
    "        return act\n",
    "\n",
    "def batch_norm_fc(m_input,num):\n",
    "    with tf.name_scope(\"batch_norm\"+num):\n",
    "        beta = tf.Variable(tf.cast(tf.constant(0,shape=[]),tf.float32),name=\"beta\"+num)\n",
    "        gamma = tf.Variable(tf.cast(tf.constant(1,shape=[]),tf.float32),name=\"sigma\"+num)\n",
    "        m,var = tf.nn.moments(m_input,axes=[0],keep_dims=True,name=\"compute_mean_var\"+num)\n",
    "        tf.summary.scalar(\"beta\",beta)\n",
    "        tf.summary.scalar(\"gamma\",gamma)\n",
    "        return tf.nn.batch_normalization(m_input,m,var,beta,gamma,variance_epsilon=.05,name=\"batch_norm\"+num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model function\n",
    "\n",
    "def model(start_learning_rate,lr_decay,batch_size,conv_count,fc_count,conv_feats,fc_feats,hparam):\n",
    "    global LOGDIR\n",
    "    global Tr_data,Tr_data_lab,Te_data,Te_data_lab\n",
    "    EPOCHS = float(len(Tr_data))/batch_size\n",
    "    if (len(conv_feats) != conv_count):\n",
    "        return\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    \n",
    "    with tf.name_scope(\"place_holder\"):\n",
    "        is_training = tf.placeholder(tf.bool,name=\"is_training\")\n",
    "        \n",
    "    with tf.name_scope(\"data_prep\"):\n",
    "        data_tr = tf.data.Dataset.from_tensor_slices((Tr_data,Tr_data_lab))\n",
    "        data_tr = data_tr.shuffle(len(Tr_data))\n",
    "        data_tr = data_tr.map(train_data_map).batch(batch_size)\n",
    "        \n",
    "        data_tr_it = tf.data.Iterator.from_structure(data_tr.output_types,data_tr.output_shapes)\n",
    "        data_tr_it_op = data_tr_it.make_initializer(data_tr)\n",
    "        \n",
    "        data_te = tf.data.Dataset.from_tensor_slices((Te_data,Te_data_lab))\n",
    "        data_te = data_te.shuffle(len(Te_data))\n",
    "        data_te = data_te.map(train_data_map)\n",
    "        \n",
    "        \n",
    "        data_te_it = tf.data.Iterator.from_structure(data_te.output_types,data_te.output_shapes)\n",
    "        data_te_it_op = data_te_it.make_initializer(data_te)\n",
    "        \n",
    "        def f1():\n",
    "            x1,y1 = data_tr_it.get_next()\n",
    "            return x1,y1\n",
    "        def f2():\n",
    "            x2,y2 = data_te_it.get_next()\n",
    "            return x2,y2\n",
    "        \n",
    "        x,y = tf.cond(is_training,lambda: data_tr_it.get_next(),lambda: data_te_it.get_next())\n",
    "        x_image = tf.reshape(x,[-1,50,50,1])\n",
    "        y_ = tf.reshape(y,[-1,2])\n",
    "    #with tf.name_scope(\"place_holders\"):\n",
    "        #x = tf.placeholder(tf.float32,shape=[50,50,1],name=\"x\")\n",
    "        #x_image = tf.reshape(x,[-1,50,50,1])\n",
    "        #y = tf.placeholder(tf.float32,shape=[None,2],name=\"y\")\n",
    "    \n",
    "    with tf.name_scope(\"variable\"):\n",
    "        learning_rt = tf.Variable(start_learning_rate,name=\"learning_rt\")\n",
    "        global_step = tf.Variable(0,trainable=False)\n",
    "        tf.summary.scalar(\"Learning_rate\",learning_rt)\n",
    "        \n",
    "    with tf.name_scope(\"lr_decay\"):\n",
    "        dec_learning_rate = tf.train.exponential_decay(learning_rt,global_step,EPOCHS,lr_decay,staircase=True)\n",
    "        \n",
    "        \n",
    "    \n",
    "    #tf.summary.image(\"image_input\",x_image,1)\n",
    "    \n",
    "    convs = []\n",
    "    #convs.append(x_image)\n",
    "    convs.append(x_image)\n",
    "    conv_name = \"conv\"\n",
    "    for i in range(0,conv_count-1):\n",
    "        convs.append(conv_layer(convs[i],conv_feats[i],conv_feats[i+1],5,5,2,2,\"conv\",str(i+1)))\n",
    "        \n",
    "    \n",
    "    shape = (convs[conv_count-1]).get_shape().as_list()\n",
    "    fc_feats[0] = shape[1]*shape[2]*conv_feats[conv_count-1]\n",
    "    flatten = tf.reshape(convs[conv_count-1],[-1,fc_feats[0]])\n",
    "    \n",
    "    fcs = []\n",
    "    fcs.append(flatten)\n",
    "    fcs_name = \"FC\"\n",
    "    for i in range(0,fc_count-1):\n",
    "        fcs.append((fc_layer(fcs[i],fc_feats[i],fc_feats[i+1],\"FC\",str(i+1))))\n",
    "    \n",
    "    \n",
    "    logts = fcs[len(fcs)-1]\n",
    "\n",
    "    with tf.name_scope(\"cross_entropy\"):\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logts,labels=y_),name=\"cross_entropy\")\n",
    "        tf.summary.scalar(\"cross_entropy\",cross_entropy)\n",
    "\n",
    "    with tf.name_scope(\"Train\"):\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_step = tf.train.AdamOptimizer(dec_learning_rate).minimize(cross_entropy)\n",
    "    \n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        pred = tf.equal(tf.argmax(logts,1),tf.argmax(y_,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(pred,tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\",accuracy)\n",
    "    \n",
    "    summ = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(LOGDIR+hparam)\n",
    "    \n",
    "    i = 1\n",
    "    cor = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(data_tr_it_op)\n",
    "\n",
    "        while(True):\n",
    "            try:\n",
    "                a,s = sess.run([train_step,summ],{is_training: True})\n",
    "                writer.add_summary(s,i)\n",
    "                i = i+1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"Finished\")\n",
    "                break\n",
    "        \n",
    "        writer.add_graph(sess.graph)\n",
    "        writer.close()\n",
    "        \n",
    "        sess.run(data_te_it_op)\n",
    "        \n",
    "        while(True):\n",
    "            try:\n",
    "                cor_pred = sess.run(pred,feed_dict={is_training: False})\n",
    "                if (cor_pred):\n",
    "                    cor = cor+1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"Finished Val\")\n",
    "                break\n",
    "        \n",
    "        print(float(cor))/len(Te_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n",
      "Finished\n",
      "Finished Val\n",
      "0.83875\n",
      "Finished\n",
      "Finished Val\n",
      "0.819375\n",
      "Finished\n",
      "Finished Val\n",
      "0.78375\n",
      "Finished\n",
      "Finished Val\n",
      "0.82\n"
     ]
    }
   ],
   "source": [
    "#Feats that worked\n",
    "#82% lnrt=1e-3 dec = .6\n",
    "#conv_feats=[1,8,10,12]\n",
    "#fc_feats=[0,300,150,50,2]\n",
    "print(len(Te_data))\n",
    "len_rt= 1e-3\n",
    "conv_count = 4\n",
    "fc_count = 5\n",
    "b_size = [20,30,40,50]\n",
    "conv_feats=[1,8,10,12]\n",
    "#fc_feats=[0,300,150,50,2]\n",
    "fc_feats=[0,300,150,50,2]\n",
    "for i in b_size:\n",
    "    model(len_rt,.6,i,conv_count,fc_count,conv_feats,fc_feats,\"testb\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
