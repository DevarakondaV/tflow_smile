{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directories\n",
    "s_dir = \"/home/vishnu/TFlow_ws/CameraProj/MLData/Smile\"\n",
    "o_dir = \"/home/vishnu/TFlow_ws/CameraProj/MLData/Other\"\n",
    "#s_dir = r\"C:\\Users\\Vishnu\\Desktop\\MLData\\Smile\"\n",
    "#o_dir = r\"C:\\Users\\Vishnu\\Desktop\\MLData\\Other\"\n",
    "\n",
    "#Log dir\n",
    "LOGDIR = \"/home/vishnu/TFlow_ws/CameraProj/src/logs\"\n",
    "#LOGDIR = r\"C:\\Users\\Vishnu\\Desktop\\MLData\\logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(Sdir,Odir):\n",
    "    filelist = [Sdir+\"/\"+x for x in os.listdir(Sdir)]+[Odir+\"/\"+y for y in os.listdir(Odir)]\n",
    "    shuffle(filelist)\n",
    "    label = [1 if x.find('Smile') != -1 else 0 for x in filelist]\n",
    "    return filelist, label\n",
    "    \n",
    "def data_aug(filenames,labels):\n",
    "    img_string = tf.read_file(filenames,name=\"data_aug_read_files\")\n",
    "    img_decode = tf.image.decode_jpeg(img_string,channels=1)\n",
    "    img = tf.cast(img_decode,tf.float32,name=\"cast_float_32\")\n",
    "    img = tf.image.per_image_standardization(img)\n",
    "    \n",
    "    mirror_img = tf.image.flip_left_right(img)\n",
    "    one_hot = tf.one_hot(labels,2)\n",
    "    \n",
    "    \n",
    "    return img,mirror_img,one_hot\n",
    "\n",
    "def train_data_map(x,y):\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up data for model\n",
    "filenames,labels = get_files(s_dir,o_dir)\n",
    "dataset_aug = tf.data.Dataset.from_tensor_slices((filenames,labels))\n",
    "dataset_aug = dataset_aug.map(data_aug)\n",
    "#dataset_aug.shuffle()\n",
    "it = tf.data.Iterator.from_structure(dataset_aug.output_types,dataset_aug.output_shapes)\n",
    "it_op = it.make_initializer(dataset_aug)\n",
    "\n",
    "\n",
    "x1,x2,yy = it.get_next()\n",
    "#ims = tf.stack([x1,x2],0)\n",
    "#labs = tf.stack([y_,y_],0)\n",
    "#btc = tf.concat([x1,y_],0)\n",
    "#Variable initialize operation\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n",
      "(6400, 50, 50, 1)\n",
      "(6400, 2)\n"
     ]
    }
   ],
   "source": [
    "img_stk = []\n",
    "label_stk = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    sess.run(it_op)\n",
    "    \n",
    "    while(True):\n",
    "        try:\n",
    "            r1,r2,r3 = sess.run([x1,x2,yy])\n",
    "            img_stk.append(r1)\n",
    "            img_stk.append(r2)\n",
    "            label_stk.append(r3)\n",
    "            label_stk.append(r3)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Finished\")\n",
    "            break\n",
    "\n",
    "print(np.asarray(img_stk).shape)\n",
    "print(np.asarray(label_stk).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800\n",
      "4800\n",
      "1600\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "#split arrayinto training and other\n",
    "length = len(img_stk)\n",
    "t_f = 3*int(length/4.0)\n",
    "\n",
    "img_stk = np.asarray(img_stk)\n",
    "label_stk=  np.asarray(label_stk)\n",
    "Tr_data = img_stk[:t_f]\n",
    "Tr_data_lab = label_stk[:t_f]\n",
    "Te_data = img_stk[t_f:]\n",
    "Te_data_lab = label_stk[t_f:]\n",
    "print(len(Tr_data))\n",
    "print(len(Tr_data_lab))\n",
    "print(len(Te_data))\n",
    "print(len(Te_data_lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.from_tensor_slices((Tr_data,Tr_data_lab))\n",
    "data = data.shuffle(len(Tr_data))\n",
    "data = data.map(train_data_map).batch(10)\n",
    "data_it = tf.data.Iterator.from_structure(data.output_types,data.output_shapes)\n",
    "data_it_op = data_it.make_initializer(data)\n",
    "x,y_ = data_it.get_next()\n",
    "\n",
    "glob_init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50, 50, 1)\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(glob_init)\n",
    "    sess.run(data_it_op)\n",
    "    \n",
    "    xx,yy = sess.run([x,y_])\n",
    "    print(np.asarray(xx).shape)\n",
    "    print(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing Data\n",
    "data_array = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    sess.run(it_op)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            rtn,yy = sess.run([x,y_])\n",
    "            data_array.append([rtn[0],yy])\n",
    "            data_array.append([rtn[1],yy])\n",
    "            \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Finished\")\n",
    "            break\n",
    "shuffle(data_array)\n",
    "\n",
    "#Splitting data into train and other\n",
    "length = len(data_array)\n",
    "third = 3*int(length/4.0)\n",
    "\n",
    "data_array = np.array(data_array)\n",
    "Train_data = data_array[:third][:,0]\n",
    "Train_data_labels = data_array[:third][:,1]\n",
    "Test_data = data_array[third:][:,0]\n",
    "Test_data_labels = data_array[third:][:,1]\n",
    "\n",
    "count = 0;\n",
    "for i in range(len(Train_data_labels)):\n",
    "    if (Train_data_labels[i][0] == 1):\n",
    "        count = count+1\n",
    "\n",
    "print(len(Train_data_labels),count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tflow functions\n",
    "\n",
    "def conv_layer(m_input,size_in,size_out,k_size_w,k_size_h,pool_k_size,pool_stride_size,name,num):\n",
    "    with tf.name_scope(name+num):\n",
    "        w = tf.Variable(tf.truncated_normal([k_size_w,k_size_h,size_in,size_out],stddev = 0.1),name=(\"w\"+num))\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[size_out]),name=(\"b\"+num))\n",
    "        \n",
    "        conv = tf.nn.conv2d(m_input,w,strides=[1,1,1,1],padding=\"SAME\")\n",
    "        act = tf.nn.leaky_relu((conv+b),alpha=0.3)\n",
    "        #.3\n",
    "        \n",
    "        tf.summary.histogram(\"weights\",w)\n",
    "        tf.summary.histogram(\"biases\",b)\n",
    "        tf.summary.histogram(\"activations\",act)\n",
    "        return tf.nn.max_pool(act,ksize=[1,pool_k_size,pool_k_size,1],strides=[1,pool_stride_size,pool_stride_size,1],padding=\"SAME\")\n",
    "    \n",
    "def fc_layer(m_input,size_in,size_out,name,num):\n",
    "    with tf.name_scope(name+num):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in,size_out],stddev=0.1),name=(\"w\"+num))\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[size_out]),name=(\"b\"+num))\n",
    "        z = tf.matmul(m_input,w)+b\n",
    "        #bt_norm = batch_norm_fc(z,num)\n",
    "        #act = tf.nn.leaky_relu(bt_norm,alpha=0.3)\n",
    "        act = tf.nn.leaky_relu(z,alpha=0.3)\n",
    "        \n",
    "        tf.summary.histogram(\"weights\",w)\n",
    "        tf.summary.histogram(\"biases\",b)\n",
    "        tf.summary.histogram(\"activations\",act)\n",
    "        return act\n",
    "\n",
    "def batch_norm_fc(m_input,num):\n",
    "    with tf.name_scope(\"batch_norm\"+num):\n",
    "        beta = tf.Variable(tf.cast(tf.constant(0,shape=[]),tf.float32),name=\"beta\"+num)\n",
    "        gamma = tf.Variable(tf.cast(tf.constant(1,shape=[]),tf.float32),name=\"sigma\"+num)\n",
    "        m,var = tf.nn.moments(m_input,axes=[0],keep_dims=True,name=\"compute_mean_var\"+num)\n",
    "        tf.summary.scalar(\"beta\",beta)\n",
    "        tf.summary.scalar(\"gamma\",gamma)\n",
    "        return tf.nn.batch_normalization(m_input,m,var,beta,gamma,variance_epsilon=.05,name=\"batch_norm\"+num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model function\n",
    "\n",
    "def model(start_learning_rate,lr_decay,conv_count,fc_count,conv_feats,fc_feats,hparam):\n",
    "    global LOGDIR\n",
    "    global Train_data\n",
    "    if (len(conv_feats) != conv_count):\n",
    "        return\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    with tf.name_scope(\"place_holders\"):\n",
    "        x = tf.placeholder(tf.float32,shape=[50,50,1],name=\"x\")\n",
    "        x_image = tf.reshape(x,[-1,50,50,1])\n",
    "        y = tf.placeholder(tf.float32,shape=[None,2],name=\"y\")\n",
    "    \n",
    "    with tf.name_scope(\"variable\"):\n",
    "        learning_rt = tf.Variable(start_learning_rate,name=\"learning_rt\")\n",
    "        global_step = tf.Variable(0,trainable=False)\n",
    "        tf.summary.scalar(\"Learning_rate\",learning_rt)\n",
    "        \n",
    "    with tf.name_scope(\"lr_decay\"):\n",
    "        dec_learning_rate = tf.train.exponential_decay(learning_rt,global_step,len(Train_data),lr_decay,staircase=True)\n",
    "        \n",
    "        \n",
    "    \n",
    "    tf.summary.image(\"image_input\",x_image,1)\n",
    "    \n",
    "    convs = []\n",
    "    convs.append(x_image)\n",
    "    conv_name = \"conv\"\n",
    "    for i in range(0,conv_count-1):\n",
    "        convs.append(conv_layer(convs[i],conv_feats[i],conv_feats[i+1],5,5,2,2,\"conv\",str(i+1)))\n",
    "    #print((conv_layer(convs[0],1,conv_feats[0],5,5,2,2,\"conv\",str(0+1)).shape))\n",
    "        \n",
    "    \n",
    "    shape = (convs[conv_count-1]).get_shape().as_list()\n",
    "    fc_feats[0] = shape[1]*shape[2]*conv_feats[conv_count-1]\n",
    "    flatten = tf.reshape(convs[conv_count-1],[1,fc_feats[0]])\n",
    "    \n",
    "    fcs = []\n",
    "    fcs.append(flatten)\n",
    "    fcs_name = \"FC\"\n",
    "    for i in range(0,fc_count-1):\n",
    "        fcs.append((fc_layer(fcs[i],fc_feats[i],fc_feats[i+1],\"FC\",str(i+1))))\n",
    "    \n",
    "    \n",
    "    logts = fcs[len(fcs)-1]\n",
    "\n",
    "    with tf.name_scope(\"cross_entropy\"):\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logts,labels=y),name=\"cross_entropy\")\n",
    "        tf.summary.scalar(\"cross_entropy\",cross_entropy)\n",
    "\n",
    "    with tf.name_scope(\"Train\"):\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_step = tf.train.AdamOptimizer(dec_learning_rate).minimize(cross_entropy)\n",
    "    \n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        pred = tf.equal(tf.argmax(logts,1),tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(pred,tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\",accuracy)\n",
    "    \n",
    "    summ = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(LOGDIR+hparam)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        cor = 0\n",
    "        for i in range(len(Train_data)):\n",
    "            l = Train_data_labels[i].reshape(1,2)\n",
    "        \n",
    "            if i % 5 == 0:\n",
    "                [train_accuracy,s] = sess.run([accuracy,summ],feed_dict={x: Train_data[i],y: l})\n",
    "                writer.add_summary(s,i)\n",
    "        \n",
    "            a = sess.run([train_step],feed_dict={x: Train_data[i],y: l})\n",
    "    \n",
    "        for i in range(len(Test_data)):\n",
    "            l = Test_data_labels[i].reshape(1,2)\n",
    "            cor_pred = sess.run(pred,feed_dict={x: Test_data[i],y: l})\n",
    "            if (cor_pred):\n",
    "                cor = cor+1\n",
    "        \n",
    "        writer.add_graph(sess.graph)   \n",
    "        writer.close()\n",
    "\n",
    "        print(float(cor)/len(Test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feats that worked\n",
    "#82% lnrt=1e-3 dec = .6\n",
    "#conv_feats=[1,8,10,12]\n",
    "#fc_feats=[0,300,150,50,2]\n",
    "print(len(Train_data))\n",
    "len_rt= 1e-3\n",
    "conv_count = 4\n",
    "fc_count = 5\n",
    "conv_feats=[1,8,10,12]\n",
    "#fc_feats=[0,300,150,50,2]\n",
    "fc_feats=[0,300,150,50,2]\n",
    "model(len_rt,.6,conv_count,fc_count,conv_feats,fc_feats,\"test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
