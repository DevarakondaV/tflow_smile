{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from random import shuffle\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Directories\n",
    "#s_dir = \"/home/vishnu/TFlow_ws/CameraProj/MLData/Smile\"\n",
    "#o_dir = \"/home/vishnu/TFlow_ws/CameraProj/MLData/Other\"\n",
    "#exp_dir = \"/home/vishnu/TFlow_ws/CameraProj/src/save/\"\n",
    "s_dir = r\"C:\\Users\\Vishnu\\Desktop\\MLData\\Smile\"\n",
    "o_dir = r\"C:\\Users\\Vishnu\\Desktop\\MLData\\Other\"\n",
    "\n",
    "#Log dir\n",
    "#LOGDIR = \"/home/vishnu/TFlow_ws/CameraProj/src/logs/\"\n",
    "LOGDIR = r\"C:\\Users\\Vishnu\\Desktop\\MLData\\logs\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_files(Sdir,Odir):\n",
    "    filelist = [Sdir+\"/\"+x for x in os.listdir(Sdir)]+[Odir+\"/\"+y for y in os.listdir(Odir)]\n",
    "    shuffle(filelist)\n",
    "    label = [1 if x.find('Smile') != -1 else 0 for x in filelist]\n",
    "    return filelist, label\n",
    "    \n",
    "def data_aug(filenames,labels):\n",
    "    img_string = tf.read_file(filenames,name=\"data_aug_read_files\")\n",
    "    img_decode = tf.image.decode_jpeg(img_string,channels=1)\n",
    "    img = tf.cast(img_decode,tf.float32,name=\"cast_float_32\")\n",
    "    img = tf.image.per_image_standardization(img)\n",
    "    \n",
    "    mirror_img = tf.image.flip_left_right(img)\n",
    "    one_hot = tf.one_hot(labels,2)\n",
    "    \n",
    "    \n",
    "    return img,mirror_img,one_hot\n",
    "\n",
    "def train_data_map(x,y):\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting up data for model\n",
    "filenames,labels = get_files(s_dir,o_dir)\n",
    "dataset_aug = tf.data.Dataset.from_tensor_slices((filenames,labels))\n",
    "dataset_aug = dataset_aug.map(data_aug)\n",
    "#dataset_aug.shuffle()\n",
    "it = tf.data.Iterator.from_structure(dataset_aug.output_types,dataset_aug.output_shapes)\n",
    "it_op = it.make_initializer(dataset_aug)\n",
    "\n",
    "\n",
    "x1,x2,yy = it.get_next()\n",
    "#ims = tf.stack([x1,x2],0)\n",
    "#labs = tf.stack([y_,y_],0)\n",
    "#btc = tf.concat([x1,y_],0)\n",
    "#Variable initialize operation\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n",
      "(6400, 50, 50, 1)\n",
      "(6400, 2)\n"
     ]
    }
   ],
   "source": [
    "img_stk = []\n",
    "label_stk = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    sess.run(it_op)\n",
    "    \n",
    "    while(True):\n",
    "        try:\n",
    "            r1,r2,r3 = sess.run([x1,x2,yy])\n",
    "            img_stk.append(r1)\n",
    "            img_stk.append(r2)\n",
    "            label_stk.append(r3)\n",
    "            label_stk.append(r3)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Finished\")\n",
    "            break\n",
    "\n",
    "print(np.asarray(img_stk).shape)\n",
    "print(np.asarray(label_stk).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800\n",
      "4800\n",
      "1600\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "#split arrayinto training and other\n",
    "length = len(img_stk)\n",
    "t_f = 3*int(length/4.0)\n",
    "\n",
    "img_stk = np.asarray(img_stk)\n",
    "label_stk=  np.asarray(label_stk)\n",
    "Tr_data = img_stk[:t_f]\n",
    "Tr_data_lab = label_stk[:t_f]\n",
    "Te_data = img_stk[t_f:]\n",
    "Te_data_lab = label_stk[t_f:]\n",
    "print(len(Tr_data))\n",
    "print(len(Tr_data_lab))\n",
    "print(len(Te_data))\n",
    "print(len(Te_data_lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tflow functions\n",
    "\n",
    "def conv_layer(m_input,size_in,size_out,k_size_w,k_size_h,pool_k_size,pool_stride_size,name,num):\n",
    "    with tf.name_scope(name+num):\n",
    "        sdev = np.power(2.0/(k_size_w*k_size_h*size_in),0.5)\n",
    "        print(\"sdev\"+num,sdev)\n",
    "        w = tf.Variable(tf.truncated_normal([k_size_w,k_size_h,size_in,size_out],stddev = sdev),name=(\"w\"+num))\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[size_out]),name=(\"b\"+num))\n",
    "        \n",
    "        conv = tf.nn.conv2d(m_input,w,strides=[1,1,1,1],padding=\"SAME\")\n",
    "        act = tf.nn.leaky_relu((conv+b),alpha=0.3)\n",
    "        #.3\n",
    "        \n",
    "        tf.summary.histogram(\"weights\",w)\n",
    "        tf.summary.histogram(\"biases\",b)\n",
    "        tf.summary.histogram(\"activations\",act)\n",
    "        return tf.nn.max_pool(act,ksize=[1,pool_k_size,pool_k_size,1],strides=[1,pool_stride_size,pool_stride_size,1],padding=\"SAME\")\n",
    "    \n",
    "def fc_layer(m_input,size_in,size_out,name,num):\n",
    "    with tf.name_scope(name+num):\n",
    "        sdev = np.power(2.0/(size_in*size_out),.5)\n",
    "        print(\"sdev\"+name+num,sdev)\n",
    "        w = tf.Variable(tf.truncated_normal([size_in,size_out],stddev=sdev),name=(\"w\"+num))\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[size_out]),name=(\"b\"+num))\n",
    "        #z = tf.matmul(m_input,w)+b\n",
    "        z = tf.matmul(m_input,w)\n",
    "        bt_norm = batch_norm_fc(z,num,.9)\n",
    "        act = tf.nn.leaky_relu(bt_norm,alpha=0.3)\n",
    "        \n",
    "        tf.summary.histogram(\"weights\",w)\n",
    "        #tf.summary.histogram(\"biases\",b)\n",
    "        tf.summary.histogram(\"activations\",act)\n",
    "        return act\n",
    "\n",
    "def batch_norm_fc(m_input,num,bn_decay):\n",
    "    with tf.name_scope(\"batch_norm\"+num):\n",
    "        \n",
    "        pop_mean = tf.Variable(tf.zeros(m_input.get_shape()[-1]),trainable=False,name=\"pop_mean\"+num)\n",
    "        pop_var = tf.Variable(tf.ones(m_input.get_shape()[-1]),trainable = False,name=\"pop_var\"+num)\n",
    "        b_beta = tf.Variable(tf.zeros(m_input.get_shape()[-1]),name=\"beta\"+num)\n",
    "        b_gamma = tf.Variable(tf.ones(m_input.get_shape()[-1]),name=\"gamma\"+num)\n",
    "        \n",
    "        tf.summary.histogram(\"beta\",b_beta)\n",
    "        tf.summary.histogram(\"gamma\",b_gamma)\n",
    "        \n",
    "        def f_true():\n",
    "            m,var = tf.nn.moments(m_input,axes=[0],name=\"comp_mvar\"+num)\n",
    "            t_mean = tf.assign(pop_mean,pop_mean*bn_decay+m*(1-bn_decay))\n",
    "            t_var = tf.assign(pop_var,pop_var*bn_decay+var*(1-bn_decay))\n",
    "            \n",
    "            with tf.control_dependencies([t_mean,t_var]):\n",
    "                return tf.nn.batch_normalization(m_input,m,var,b_beta,b_gamma,variance_epsilon=.05,name=\"bn_train\"+num)\n",
    "        \n",
    "        def f_false():\n",
    "            return tf.nn.batch_normalization(m_input,pop_mean,pop_var,b_beta,b_gamma,variance_epsilon=.05,name=\"bn_test\"+num)\n",
    "        \n",
    "        rtn_ten = tf.cond(tf.get_default_graph().get_tensor_by_name(\"place_holder/is_training:0\"),f_true,f_false)\n",
    "        return rtn_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model function\n",
    "rdom = np.random.rand(50,50).reshape(1,50,50,1)\n",
    "def model(start_learning_rate,lr_decay,batch_size,conv_count,fc_count,conv_feats,fc_feats,hparam):\n",
    "    global LOGDIR\n",
    "    global Tr_data,Tr_data_lab,Te_data,Te_data_lab,rdom\n",
    "    EPOCHS = float(len(Tr_data))/batch_size\n",
    "    if (len(conv_feats) != conv_count):\n",
    "        return\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    g_step = tf.train.create_global_step()\n",
    "    \n",
    "    with tf.name_scope(\"place_holder\"):\n",
    "        is_training = tf.placeholder(tf.bool,name=\"is_training\")\n",
    "        is_pred = tf.placeholder(tf.bool,name=\"is_pred\")\n",
    "        xx = tf.placeholder(tf.float32,shape=[1,50,50,1],name=\"xx\")\n",
    "        #global_step = tf.placeholder(tf.int32,name=\"glob_step\")\n",
    "        \n",
    "    with tf.name_scope(\"data_prep\"):\n",
    "        data_tr = tf.data.Dataset.from_tensor_slices((Tr_data,Tr_data_lab))\n",
    "        data_tr = data_tr.shuffle(len(Tr_data))\n",
    "        data_tr = data_tr.map(train_data_map).batch(batch_size)\n",
    "        \n",
    "        data_tr_it = tf.data.Iterator.from_structure(data_tr.output_types,data_tr.output_shapes)\n",
    "        data_tr_it_op = data_tr_it.make_initializer(data_tr)\n",
    "        \n",
    "        data_te = tf.data.Dataset.from_tensor_slices((Te_data,Te_data_lab))\n",
    "        data_te = data_te.shuffle(len(Te_data))\n",
    "        data_te = data_te.map(train_data_map)\n",
    "        \n",
    "        \n",
    "        data_te_it = tf.data.Iterator.from_structure(data_te.output_types,data_te.output_shapes)\n",
    "        data_te_it_op = data_te_it.make_initializer(data_te)\n",
    "        \n",
    "        def f1():\n",
    "            x1,y1 = data_tr_it.get_next()\n",
    "            return x1,y1\n",
    "        def f2():\n",
    "            x2,y2 = data_te_it.get_next()\n",
    "            return x2,y2\n",
    "        \n",
    "        x,y = tf.cond(is_training,lambda: data_tr_it.get_next(),lambda: data_te_it.get_next())\n",
    "        xxx = tf.cond(is_pred,lambda: xx,lambda: x)\n",
    "        x_image = tf.reshape(xxx,[-1,50,50,1])\n",
    "        y_ = tf.reshape(y,[-1,2])\n",
    "    \n",
    "    with tf.name_scope(\"variable\"):\n",
    "        learning_rt = tf.Variable(start_learning_rate,name=\"learning_rt\")\n",
    "        \n",
    "    with tf.name_scope(\"lr_decay\"):\n",
    "        dec_learning_rate = tf.train.exponential_decay(learning_rt,g_step,batch_size,lr_decay,staircase=True)\n",
    "        tf.summary.scalar(\"learning_rt\",dec_learning_rate)\n",
    "        \n",
    "    \n",
    "    #tf.summary.image(\"image_input\",x_image,1)\n",
    "    \n",
    "    convs = []\n",
    "    #convs.append(x_image)\n",
    "    convs.append(x_image)\n",
    "    conv_name = \"conv\"\n",
    "    for i in range(0,conv_count-1):\n",
    "        convs.append(conv_layer(convs[i],conv_feats[i],conv_feats[i+1],5,5,2,2,\"conv\",str(i+1)))\n",
    "        \n",
    "    \n",
    "    shape = (convs[conv_count-1]).get_shape().as_list()\n",
    "    fc_feats[0] = shape[1]*shape[2]*conv_feats[conv_count-1]\n",
    "    flatten = tf.reshape(convs[conv_count-1],[-1,fc_feats[0]])\n",
    "    \n",
    "    fcs = []\n",
    "    fcs.append(flatten)\n",
    "    fcs_name = \"FC\"\n",
    "    for i in range(0,fc_count-1):\n",
    "        fcs.append((fc_layer(fcs[i],fc_feats[i],fc_feats[i+1],\"FC\",str(i+1))))\n",
    "    \n",
    "    \n",
    "    logts = fcs[len(fcs)-1]\n",
    "\n",
    "    with tf.name_scope(\"cross_entropy\"):\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logts,labels=y_),name=\"cross_entropy\")\n",
    "        tf.summary.scalar(\"cross_entropy\",cross_entropy)\n",
    "\n",
    "    with tf.name_scope(\"Train\"):\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_step = tf.train.AdamOptimizer(dec_learning_rate).minimize(cross_entropy,global_step=g_step)\n",
    "    \n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        pred = tf.equal(tf.argmax(logts,1),tf.argmax(y_,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(pred,tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\",accuracy)\n",
    "    \n",
    "    summ = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(LOGDIR+hparam)\n",
    "    #builder = tf.saved_model.builder.SavedModelBuilder(exp_dir)\n",
    "    #pred_tensor_inputs_info = tf.saved_model.utils.build_tensor_info(xx)\n",
    "    #pred_tensor_is_tr_info = tf.saved_model.utils.build_tensor_info(is_training)\n",
    "    #pred_tensor_is_pred_info = tf.saved_model.utils.build_tensor_info(is_pred)\n",
    "    #pred_tensor_output_info = tf.saved_model.utils.build_tensor_info(logts)\n",
    "    #pred_sig = tf.saved_model.signature_def_utils.build_signature_def(inputs={\"image\": pred_tensor_inputs_info,\n",
    "     #                                                                        \"is_training\": pred_tensor_is_tr_info,\n",
    "      #                                                                       \"is_pred\": pred_tensor_is_pred_info},\n",
    "       #                                                              outputs={\"score\": pred_tensor_output_info},\n",
    "        #                                                             method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "    #legacy_init_op = tf.group(tf.tables_initializer(),name='legacy_init_op')\n",
    "    \n",
    "    \n",
    "    i = 1\n",
    "    cor = 0\n",
    "    rtnval = []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(data_tr_it_op)\n",
    "        #tsor = tf.get_default_graph().get_tensor_by_name(\"FC1/w1:0\")\n",
    "        #rtnval.append(sess.run(tsor))\n",
    "        \n",
    "        for i in range(0,10):\n",
    "            sess.run(data_tr_it_op)\n",
    "            while(True):\n",
    "                try:\n",
    "                    a,s = sess.run([train_step,summ],{is_training: True,is_pred: False,xx: rdom})\n",
    "                    writer.add_summary(s,i)\n",
    "                    #i = i+1\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    print(\"Finished\"+\" \"+str(i))\n",
    "                    break\n",
    "        \n",
    "        writer.add_graph(sess.graph)\n",
    "        writer.close()\n",
    "        \n",
    "\n",
    "        \n",
    "        #rtnval.append(sess.run(tsor))\n",
    "        sess.run(data_te_it_op)\n",
    "        \n",
    "        while(True):\n",
    "            try:\n",
    "                cor_pred, lg, yy= sess.run([pred,logts,y_],feed_dict={is_training: False, is_pred: False,xx: rdom})\n",
    "                if (cor_pred):\n",
    "                    cor = cor+1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"Finished Val\")\n",
    "                break\n",
    "        \n",
    "        print(hparam,float(cor)/len(Te_data))\n",
    "        \n",
    "        \n",
    "        #builder.add_meta_graph_and_variables(sess,[tf.saved_model.tag_constants.SERVING],\n",
    "        #                                    signature_def_map={'predict_images':pred_sig},\n",
    "        #                                    legacy_init_op=legacy_init_op)\n",
    "        #builder.save()\n",
    "    return rtnval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdev1 0.282842712475\n",
      "sdev2 0.1\n",
      "sdev3 0.0707106781187\n",
      "sdevFC1 0.00206196524711\n",
      "sdevFC2 0.00666666666667\n",
      "sdevFC3 0.0163299316186\n",
      "sdevFC4 0.141421356237\n",
      "Finished 0\n",
      "Finished 1\n",
      "Finished 2\n",
      "Finished 3\n",
      "Finished 4\n",
      "Finished 5\n",
      "Finished 6\n",
      "Finished 7\n",
      "Finished 8\n",
      "Finished 9\n",
      "Finished Val\n",
      "test_b_30_dr_.75 0.874375\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'builder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d6d79f821352>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#    for j in dec_rt:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#        model(len_rt,j,i,conv_count,fc_count,conv_feats,fc_feats,\"testb\"+str(i)+str(\"dec\")+str(j))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen_rt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m.75\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconv_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfc_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconv_feats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfc_feats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"test_b_30_dr_.75\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-79d55668d165>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(start_learning_rate, lr_decay, batch_size, conv_count, fc_count, conv_feats, fc_feats, hparam)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         builder.add_meta_graph_and_variables(sess,[tf.saved_model.tag_constants.SERVING],\n\u001b[0m\u001b[0;32m    148\u001b[0m                                             \u001b[0msignature_def_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'predict_images'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpred_sig\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                                             legacy_init_op=legacy_init_op)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'builder' is not defined"
     ]
    }
   ],
   "source": [
    "#Feats that worked\n",
    "#82% lnrt=1e-3 dec = .8 bs = 30\n",
    "#conv_feats=[1,8,10,12]\n",
    "#fc_feats=[0,300,150,50,2]\n",
    "len_rt= 1e-2\n",
    "conv_count = 4\n",
    "fc_count = 5\n",
    "conv_feats=[1,8,16,32]\n",
    "fc_feats=[0,300,150,50,2]\n",
    "#.6....25\n",
    "#.8....30\n",
    "#for i in b_size:\n",
    "#    for j in dec_rt:\n",
    "#        model(len_rt,j,i,conv_count,fc_count,conv_feats,fc_feats,\"testb\"+str(i)+str(\"dec\")+str(j))\n",
    "r = model(len_rt,.75,30,conv_count,fc_count,conv_feats,fc_feats,\"test_b_30_dr_.75\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
